{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ],
      "metadata": {
        "id": "dqZcndSSjtkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -O ./spark-3.3.1-bin-hadoop3.tgz  https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
        "!tar zxvf ./spark-3.3.1-bin-hadoop3.tgz \n",
        "!pip install findspark "
      ],
      "metadata": {
        "id": "us9wYFRtjwvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "2Fre6_SsjwrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import random\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "JWpGM95bj25y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('lr_example').getOrCreate()"
      ],
      "metadata": {
        "id": "zSq4H5o7Y5X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pyspark\n",
        "from pyspark.rdd import RDD\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, IntegerType, StringType\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import substring, when, concat, lit, col, to_date, regexp_replace, udf\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") #Ignores all unfamiliar fonts\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (15,7)\n",
        "\n",
        "import datetime"
      ],
      "metadata": {
        "id": "IDexHskEgkky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data and Translate the data"
      ],
      "metadata": {
        "id": "sIp03wWjN2mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "ldPNGjW01iAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# link = 'https://drive.google.com/file/d/1d8b_n-6fo5HbxyW-kwY3c8fWlvLOcFYy/view?usp=share_link'\n",
        " \n",
        "# # to get the id part of the file\n",
        "# id = link.split(\"/\")[-2]\n",
        " \n",
        "# downloaded = drive.CreateFile({'id':id})\n",
        "# downloaded.GetContentFile('-3-2022-.csv') \n",
        " \n",
        "# df_pd = pd.read_csv('-3-2022-.csv')\n",
        "# df_pd.info()"
      ],
      "metadata": {
        "id": "CIINuCnQ1hnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In this section we translated the data from Hebrew to English. Then we downloaded the translated file and opened it with pyspark."
      ],
      "metadata": {
        "id": "JTRm8_9ZhamG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install googletrans==4.0.0rc1 gwpy &> /dev/null"
      ],
      "metadata": {
        "id": "MFYmGojeN8xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from googletrans import Translator\n",
        "# translator = Translator()"
      ],
      "metadata": {
        "id": "xZwTFY1eQbDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def translate_column (df:pd.DataFrame, name:str):\n",
        "#   y = 'he_' + name\n",
        "#   column_to_translate = pd.DataFrame(list(df[name].unique())).rename({0:y},axis=1)\n",
        "#   x = 'en_' + name\n",
        "#   column_to_translate[x] = column_to_translate[y].apply(lambda x: translator.translate(x, src='he', dest='en').text)\n",
        "#   df = df.merge(column_to_translate, left_on=name, right_on=y,how ='inner')\n",
        "#   df[name]=df[x]\n",
        "#   df=df.drop(columns=[x,y])\n",
        "#   return df"
      ],
      "metadata": {
        "id": "Ouu10Xk23lov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en_df_pd = df_pd\n",
        "# column_names = en_df_pd.columns\n",
        "# k = column_names.delete([4,5,6,9])\n",
        "# for i in k:\n",
        "#   en_df_pd = translate_column(en_df_pd,i)\n",
        "# en_df_pd"
      ],
      "metadata": {
        "id": "gGEmIX9LEb1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# en_df_pd.to_csv('/content/drive/MyDrive/BDP_Project/en_df.csv', index=False)"
      ],
      "metadata": {
        "id": "0U1f-tzIqeXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Spark-DF"
      ],
      "metadata": {
        "id": "VXhwSQOybU9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "lhun1Dw6c4fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = 'https://drive.google.com/file/d/1lUTnYWvVjtoHVkiH3whj-dREJDdPoez2/view?usp=share_link'\n",
        " \n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        " \n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('en_df.csv') \n",
        " \n",
        "df = spark.read.csv(\"en_df.csv\",inferSchema=True, header=True)\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "#'df' show the translate data"
      ],
      "metadata": {
        "id": "OhRDHgcAXQyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixed a wrong translation\n",
        "df = df.withColumn('PoliceDistrict', regexp_replace('PoliceDistrict', 'Cell County', 'Tel Aviv District'))\n",
        "df = df.withColumn('PoliceDistrict', regexp_replace('PoliceDistrict', 'Shay County', 'Judea and Samaria District'))"
      ],
      "metadata": {
        "id": "C54w3Y26j0X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the attribute from 'double' to 'int'\n",
        "df1 = df.withColumn(\"TikimSum\",df.TikimSum.cast('int'))"
      ],
      "metadata": {
        "id": "d1uTqoZ2ukiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In the next two cells we merged the date columns(quarter and year) to one column and Change the new attribute to 'date'\n",
        "df2 = df1.withColumn('year', substring('Quarter', 1,4))\\\n",
        "    .withColumn('Q', substring('Quarter', 6,2))\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "id": "MVVcA_rj2-Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.withColumn(\"year\",df2.year.cast(StringType()))\n",
        "df3 = df2.withColumn(\"date\", \\\n",
        "   when((df2.Q == 'Q2'), lit(\"-30-06\"))\\\n",
        "   .when((df2.Q == 'Q3'), lit(\"-30-09\"))\\\n",
        "   .when((df2.Q == 'Q4'), lit(\"-31-12\"))\\\n",
        "   .otherwise(lit(\"-31-03\")) \\\n",
        "  )\n",
        "df3 = df3.withColumn(\"Exact Date\", concat('year','date'))\n",
        "df3 = df3.withColumn(\"Date Value\", to_date(df3['Exact Date'], \"yyyy-dd-MM\"))\n",
        "df3.show(5)\n",
        "df3.printSchema()"
      ],
      "metadata": {
        "id": "jyWmBiXbUokh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = \"https://drive.google.com/file/d/1RBkuTYpTQHl0EjhiteL3DjSLd5xUxvYa/view?usp=share_link\"\n",
        " \n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        " \n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('city_data_.csv') \n",
        " \n",
        "citydata = spark.read.csv(\"city_data_.csv\",inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "HtcPUxLjvvlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'citydata' used for the population and for the names(in english) of the cities\n",
        "citydata = citydata.withColumnRenamed(\"שם יישוב\",\"Settlement_Council\")\n",
        "citydata = citydata.withColumnRenamed(\"סך הכל אוכלוסייה 2021\",\"Population\")\n",
        "citydata = citydata.withColumnRenamed(\"תעתיק\" , \"City Name\")\n",
        "\n",
        "citydata.show(5)"
      ],
      "metadata": {
        "id": "xcdHlnBOxDSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_df = citydata.select(\"Settlement_Council\", \"Population\",\"City Name\" )\n",
        "\n",
        "# Join the selected columns from city_df with df3(main data) on the 'Settlement_Council' column\n",
        "df4 = df3.join(city_df, on='Settlement_Council', how='left')"
      ],
      "metadata": {
        "id": "oIPmsFpjxWqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = \"https://drive.google.com/file/d/1FRVWLJLf_h-vWivkEzvC5RJBiI5kRnPW/view?usp=share_link\"\n",
        " \n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        " \n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('s_e_2019.csv') \n",
        " \n",
        "socio_economic_df = spark.read.csv(\"s_e_2019.csv\",inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "uhv-HyVO1i4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#'socio_economic_df' used for the Socio-economic of the cities\n",
        "#Socio-economic '1' is the lowest\n",
        "#Socio-economic '10' is the highest\n",
        "socio_economic_df.show(5)"
      ],
      "metadata": {
        "id": "sRk7LkoU2A68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "socio_economic_df = socio_economic_df.select(col(\"Settlement_Council\"), col(\"City_Cluster_2019\"))\n",
        "\n",
        "# Join the selected columns from socio_economic_df with df3(main data) on the 'Settlement_Council' column\n",
        "df5 = df4.join(socio_economic_df, on='Settlement_Council', how='left')"
      ],
      "metadata": {
        "id": "v65gXa382Rhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#After merged the two data to the main data we define the attribute 'City Name' to be 'Settlement_Council'(=the name of the cities in english)\n",
        "df5 = df5.withColumnRenamed(\"Settlement_Council\", \"Settlement_Council_Hebrew\")\n",
        "df5 = df5.withColumnRenamed(\"City Name\", \"Settlement_Council\")\n"
      ],
      "metadata": {
        "id": "NHaJm0nJaPdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_df = city_df.withColumnRenamed(\"Settlement_Council\", \"Settlement_Council_Hebrew\")\n",
        "city_df = city_df.withColumnRenamed(\"City Name\", \"Settlement_Council\")"
      ],
      "metadata": {
        "id": "791ZRfDe78Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.printSchema()\n",
        "df5.show(truncate=False)"
      ],
      "metadata": {
        "id": "lswWB69tcpAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we finished with Database modification and we start with the analysis"
      ],
      "metadata": {
        "id": "pnScov91Uxhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Analysis"
      ],
      "metadata": {
        "id": "uPEluTRgbQ-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df_Tikim_City show for every city the Socio-economic number and the number of cases per humans\n",
        "df_Tikim_City = df5.groupBy('Settlement_Council').sum('TikimSum')\n",
        "df_Tikim_City = df_Tikim_City.join(city_df, on='Settlement_Council', how='left')\n",
        "df_Tikim_City = df_Tikim_City.join(df5.select('Settlement_Council','City_Cluster_2019').distinct(),on='Settlement_Council', how='left')\n",
        "df_Tikim_City = df_Tikim_City.withColumn(\"Tikim by population\",col(\"sum(TikimSum)\")/ col(\"Population\"))\n",
        "df_Tikim_City = df_Tikim_City.sort(\"Tikim by population\",ascending=False)\n",
        "df_Tikim_City.select(\"Settlement_Council\",\"City_Cluster_2019\",\"Tikim by population\").show(10)"
      ],
      "metadata": {
        "id": "9g7s-2qxbXxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # If the cell below doesn't work, run this:\n",
        "!python -m pip uninstall matplotlib\n",
        "!pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "MdmaZp1tQUoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the distribution of 'number of cases per human'\n",
        "sns.distplot(df_Tikim_City.toPandas()[\"Tikim by population\"]);"
      ],
      "metadata": {
        "id": "FFISMVtOP8qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_Tikim_District represent for any 'PoliceDistrict' the number of cases\n",
        "df_Tikim_District = df5.groupBy('PoliceDistrict','Date Value').sum('TikimSum')"
      ],
      "metadata": {
        "id": "ejAd4npBkWLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_Tikim_District = df_Tikim_District.toPandas()"
      ],
      "metadata": {
        "id": "lVNjiA3zd1BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this graph show the distribution, by date, the number of case for any 'PoliceDistrict'\n",
        "districts = pd.unique(pd_Tikim_District['PoliceDistrict'])\n",
        "for d in districts:\n",
        "  pd_district = pd_Tikim_District[pd_Tikim_District['PoliceDistrict'] == d]\n",
        "  pd_district = pd_district.sort_values(by = 'Date Value')\n",
        "  plt.plot(pd_district['Date Value'],pd_district['sum(TikimSum)'])\n",
        "plt.legend(districts, loc=\"upper left\")\n",
        "plt.axvline(x=datetime.date(2020,4,8), color='black', linestyle='dashed')\n",
        "plt.axvline(x=datetime.date(2021,3,31), color='black', linestyle='dashed')\n",
        "plt.ylim([0,25000])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Emv9ewxNltPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tikim_district = df5.groupBy('PoliceDistrict','StatisticCrimeGroup','Date Value').sum('TikimSum')"
      ],
      "metadata": {
        "id": "FReR4XuuFZz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Every graph below represents a district\n",
        "# in every district we show the distribution, by date, the number of case for any 'StatisticCrimeGroup'\n",
        "district = [\"Southern District\", \"Judea and Samaria District\", \"Jerusalem District\", \"North District\", \"Beach district\", \"Central district\", \"Tel Aviv District\"]\n",
        "for i in range(7):\n",
        "  pd_tikim_district = df_tikim_district.where(df_tikim_district.PoliceDistrict == district[i]).toPandas()\n",
        "  types = pd.unique(pd_tikim_district['StatisticCrimeGroup'])\n",
        "  types = np.delete(types, np.where(types == \"Nan\"))\n",
        "  for t in types:\n",
        "    pd_type = pd_tikim_district[pd_tikim_district['StatisticCrimeGroup'] == t]\n",
        "    pd_type = pd_type.sort_values(by = 'Date Value')\n",
        "    plt.plot(pd_type['Date Value'],pd_type['sum(TikimSum)'])\n",
        "  plt.legend(types, loc = \"upper left\")\n",
        "  plt.axvline(x=datetime.date(2020,4,8), color='black', linestyle='dashed')\n",
        "  plt.axvline(x=datetime.date(2021,3,31), color='black', linestyle='dashed')\n",
        "  plt.title(district[i])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "On1CwJmWI_JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establishing \"Crime Score\""
      ],
      "metadata": {
        "id": "MYW2895Febyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crime_types = df5.select('StatisticCrimeGroup').distinct()\n",
        "crime_types = crime_types.withColumn('Score',col('StatisticCrimeGroup'))\n",
        "df5.select('StatisticCrimeGroup').distinct().show(17,False)"
      ],
      "metadata": {
        "id": "Z-PQ2yntpY3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We are define for every 'Crime Group' a score\n",
        "crime_scores = {'The rest of the offenses':'1', 'Traffic violations': '2', 'Nan': '3', 'Permitted offenses': '1', 'Offenses against a person':'5', 'Confidence offenses': '3', 'Sex offenses': '5',\\\n",
        "                'Administrative offenses': '1', 'Economic offenses': '2', 'Public order offenses': '1', 'Offenses toward property': '3','Setup Sections': '1',\\\n",
        "                'Fraudulent offenses': '3','Offenses against body': '5','Offenses toward morality': '4','Unknown': '3', 'The rest of the rest': '1'}\n",
        "crime_types_score = crime_types.replace(crime_scores,subset=['Score'])\n",
        "crime_types_score.sort('Score',ascending=False).show(17,False)\n"
      ],
      "metadata": {
        "id": "eICUdvyMRpUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$P = $ the population number of the city\n",
        "\n",
        "$n_i = $ number of specific crime group cases\n",
        "\n",
        "$S_i = $ the score of the specific crime group\n",
        "\n",
        "$$CrimeScore = \\frac{1}{P}\\sum_{i}{n_{i}\\cdot S_{i}}$$"
      ],
      "metadata": {
        "id": "MXMiCPrvifd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df6 = df5.join(crime_types_score, on='StatisticCrimeGroup', how='left')\n",
        "df6 = df6.withColumn(\"Total_Score\",col(\"TikimSum\") * col(\"Score\"))\n",
        "df6 = df6.groupBy('Settlement_Council').sum('Total_Score')\n",
        "df_score_city = df5.join(df6, on='Settlement_Council', how='left')\n",
        "df_score_city = df_score_city.withColumn(\"Score per population\",col(\"sum(Total_Score)\") / col(\"Population\"))"
      ],
      "metadata": {
        "id": "azqIPQIt1MgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'df_score_city' show for every city the 'Score per population'\n",
        "# The higher the score, the higher the level of crime in the city\n",
        " \n",
        "df_score_city.select(\"Settlement_Council\",'City_Cluster_2019',\"Score per population\").distinct().sort(\"Score per population\",ascending=False).show(10)"
      ],
      "metadata": {
        "id": "UIcgqsKR4zNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this graph show the distribution, by Socio-economic, the 'Score per population' per every city. in addition we try to see if there is some correlation\n",
        "pd_score = df_score_city.select('Settlement_Council','City_Cluster_2019','Score per population').distinct().toPandas()\n",
        "sns.regplot(pd_score['City_Cluster_2019'],pd_score['Score per population'],ci=0,line_kws={\"color\": \"red\"})"
      ],
      "metadata": {
        "id": "zvXAdDpd71Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#show the distribution of 'Score per population'\n",
        "sns.distplot(pd_score['Score per population']);"
      ],
      "metadata": {
        "id": "h5QMAg0vPEem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$N = $ overall number of cases in the city \n",
        "\n",
        "$n_i = $ number of specific crime group cases\n",
        "\n",
        "$S_i = $ the score of the specific crime group\n",
        "\n",
        "$$CrimeScore = \\frac{1}{N}\\sum_{i}{n_{i}\\cdot S_{i}}$$"
      ],
      "metadata": {
        "id": "bIvm73tVf2SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use another formula for the crime score\n",
        "df7 = df5.join(crime_types_score, on='StatisticCrimeGroup', how='left')\n",
        "df7 = df7.withColumn(\"Total_Score\",col(\"TikimSum\") * col(\"Score\"))\n",
        "df8 = df7.groupBy('Settlement_Council').sum('Total_Score')\n",
        "df9 = df7.groupBy('Settlement_Council').sum('TikimSum')\n",
        "df_score_city2 = df9.join(df8, on='Settlement_Council', how='left')\n",
        "df_score_city2 = df_score_city2.withColumn(\"Score per population2\",col(\"sum(Total_Score)\") / col(\"sum(TikimSum)\"))"
      ],
      "metadata": {
        "id": "91_hzObB_5F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'df_score_city2' show for every city the 'Score per population'\n",
        "# The higher the score, the higher the level of crime in the city\n",
        "df_score_city2.select(\"Settlement_Council\",\"Score per population2\").distinct().sort(\"Score per population2\",ascending=False).show(10)"
      ],
      "metadata": {
        "id": "gS3H8SKeAqXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_score_city2 = df_score_city2.join(df5.select('Settlement_Council','City_Cluster_2019'), on='Settlement_Council', how='left')"
      ],
      "metadata": {
        "id": "dxk3_pLHBXTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this graph show the distribution, by Socio-economic, the 'Score per population2' per every city. in addition we try to see if there is some correlation\n",
        "pd_score2 = df_score_city2.select('Settlement_Council','City_Cluster_2019','Score per population2').distinct().toPandas()\n",
        "sns.regplot(pd_score2['City_Cluster_2019'],pd_score2['Score per population2'],ci=0,line_kws={\"color\": \"red\"})"
      ],
      "metadata": {
        "id": "Gx3wJedBBX0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$P = $ the population number of the city\n",
        "\n",
        "$N = $ overall number of cases in the city \n",
        "\n",
        "$n_i = $ number of specific crime group cases\n",
        "\n",
        "$S_i = $ the score of the specific crime group\n",
        "\n",
        "$$CrimeScore = \\frac{1}{2} \\cdot (\\frac{1}{P}\\sum_{i}{n_{i}\\cdot S_{i}} + \\frac{1}{N}\\sum_{i}{n_{i}\\cdot S_{i}})$$\n"
      ],
      "metadata": {
        "id": "oHMBDqLiQm3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we average the two previous formulas into one and show graph as before\n",
        "dft1 = df_score_city.select(\"Settlement_Council\",\"Score per population\",'City_Cluster_2019').distinct()\n",
        "dft2 = df_score_city2.select(\"Settlement_Council\",\"Score per population2\").distinct()\n",
        "df_score_city3 = dft1.join(dft2, on=\"Settlement_Council\", how=\"left\")\n",
        "df_score_city3 = df_score_city3.withColumn(\"Average Crime Score\",(col(\"Score per population\") + col(\"Score per population2\"))/2)\n",
        "pd_score3 = df_score_city3.select('Settlement_Council', 'City_Cluster_2019','Average Crime Score').distinct().toPandas()\n",
        "sns.regplot(pd_score3['City_Cluster_2019'],pd_score3['Average Crime Score'],ci=0,line_kws={\"color\": \"red\"})"
      ],
      "metadata": {
        "id": "bkTqf2e3Oi3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(pd_score3['Average Crime Score']);"
      ],
      "metadata": {
        "id": "zaTsbFusedwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'df_score_city3' show for every city the 'Score per population'\n",
        "# The higher the score, the higher the level of crime in the city\n",
        "df_score_city3.select(\"Settlement_Council\",\"City_Cluster_2019\",\"Average Crime Score\").distinct().sort(\"Average Crime Score\",ascending=False).show(10)"
      ],
      "metadata": {
        "id": "PQld6n2veoeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deeper analysis and prediction"
      ],
      "metadata": {
        "id": "B7txVhQwjBXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df5.show()"
      ],
      "metadata": {
        "id": "GwpDPBnS3LJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building data that will fit prediction over time.\n",
        "# Each city is represented in one row\n",
        "years = ['2017', '2018', '2019', '2020', '2021', '2022']\n",
        "quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
        "df_ml = df5.select(\"Settlement_Council\",\"Population\",\"City_Cluster_2019\").distinct().dropna()\n",
        "for year in years:\n",
        "  for q in quarters:\n",
        "    if year == '2022' and q == 'Q4': # Skipping because there is no data for this time\n",
        "      break\n",
        "    year_q = year+\"_\"+q\n",
        "    df_by_time = df5.groupBy('Settlement_Council','year','Q').sum('TikimSum')\n",
        "    df_by_time = df_by_time.withColumnRenamed('sum(TikimSum)',year_q)\n",
        "    df_temp = df_by_time.where(df_by_time.year == year)\n",
        "    df_temp = df_temp.where(df_by_time.Q == q)\n",
        "    df_ml = df_ml.join(df_temp.select('Settlement_Council',year_q),on='Settlement_Council', how='left')"
      ],
      "metadata": {
        "id": "n0uUhr_x2zXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml.show(5)"
      ],
      "metadata": {
        "id": "5MY0CnqJ_DTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_columns = df_ml.columns[3:-1] # Removing the first three columns and the last quarter of 2022 (\"2022-Q3\") from the training columns"
      ],
      "metadata": {
        "id": "NIGz-1vixZC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a feature vector by combining the columns\n",
        "df_ml = df_ml.dropna()\n",
        "vector_assembler = VectorAssembler(inputCols=training_columns, outputCol=\"features\", handleInvalid=\"keep\")\n",
        "df_ml_vec = vector_assembler.transform(df_ml)"
      ],
      "metadata": {
        "id": "q5FDP6d5vMfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "df_ml_data = df_ml_vec.select(\"Settlement_Council\",\"features\",\"2022_Q3\")\n",
        "training_data, testing_data = df_ml_data.randomSplit([0.8, 0.2])"
      ],
      "metadata": {
        "id": "zSMuHszwvRVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the linear regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"2022_Q3\")\n",
        "model = lr.fit(training_data)"
      ],
      "metadata": {
        "id": "-aSRhiJgy6Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the model to make predictions on the testing data\n",
        "predictions = model.transform(testing_data)\n",
        "predictions.show(5)"
      ],
      "metadata": {
        "id": "kQA3Y1jY0LWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the accuracy of the prediction by 'r2 metric value'\n",
        "reg_eval = RegressionEvaluator(predictionCol='prediction', labelCol='2022_Q3', metricName='r2')\n",
        "reg_eval.evaluate(predictions)"
      ],
      "metadata": {
        "id": "OKuAAkWvmqjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023 = df_ml.dropna()\n",
        "training_columns = df_ml.columns[4:] #  Now we will look at data from one quarter later to adjust the model.\n",
        "vector_assembler_2023 = VectorAssembler(inputCols=training_columns, outputCol=\"features\" ,handleInvalid=\"keep\")\n",
        "df_2023 = vector_assembler_2023.transform(df_2023).select(\"Settlement_Council\",\"features\",\"2022_Q3\")\n",
        "predictions_2023 = model.transform(df_2023)"
      ],
      "metadata": {
        "id": "noFKyR8xHYlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_2023 = predictions_2023.withColumn(\"Crime Increase\",(col(\"prediction\")-col(\"2022_Q3\"))/col(\"2022_Q3\"))"
      ],
      "metadata": {
        "id": "cx6xKAhrJIU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_2023 = predictions_2023.join(df5.select(\"Settlement_Council\",\"City_Cluster_2019\").distinct(), on=\"Settlement_Council\", how=\"left\")"
      ],
      "metadata": {
        "id": "SYMW3TIVYRa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The cities with the predicted crime increase for 2023:\")\n",
        "predictions_2023.sort(\"Crime Increase\",ascending=False).select(\"Settlement_Council\",\"City_Cluster_2019\",\"Crime Increase\").show()"
      ],
      "metadata": {
        "id": "aKAnXoTcKbqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The cities with the predicted crime decrease for 2023:\")\n",
        "predictions_2023.sort(\"Crime Increase\",ascending=True).select(\"Settlement_Council\",\"City_Cluster_2019\",\"Crime Increase\").show()"
      ],
      "metadata": {
        "id": "p0a5aKyectMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RhxkHsef3unn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}