{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BBfUroneFmVT",
        "qP-tclhENY_K",
        "J4Zz9RDQmPUO",
        "DlX48EsljI-f",
        "iYrsXJVZqdsQ",
        "YT-ceD4JsDll",
        "p8SF5yQ1gdn-"
      ],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yanivt16/BDP_project/blob/main/Yaniv_Copy_of_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ],
      "metadata": {
        "id": "dqZcndSSjtkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -O ./spark-3.3.1-bin-hadoop3.tgz  https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
        "!tar zxvf ./spark-3.3.1-bin-hadoop3.tgz \n",
        "!pip install findspark "
      ],
      "metadata": {
        "id": "us9wYFRtjwvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "2Fre6_SsjwrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import random\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "JWpGM95bj25y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('lr_example').getOrCreate()"
      ],
      "metadata": {
        "id": "zSq4H5o7Y5X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other imports\n",
        "import numpy as np\n",
        "import pyspark\n",
        "from pyspark.rdd import RDD\n",
        "from pyspark import SparkContext\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, IntegerType, StringType\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import substring\n",
        "from pyspark.sql.functions import when\n",
        "from pyspark.sql.functions import concat, lit, col\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")#Ignores all unfamiliar fonts\n",
        "!pip install matplotlib==3.1.3\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (15,7)\n",
        "import datetime\n",
        "\n"
      ],
      "metadata": {
        "id": "IDexHskEgkky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data"
      ],
      "metadata": {
        "id": "NFLD1VuQ-ixV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pd = pd.read_csv('/content/-3-2022-.csv')"
      ],
      "metadata": {
        "id": "i49Rl4eOmoID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        " \n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        " \n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "metadata": {
        "id": "sSKFnYnl_FnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = 'https://drive.google.com/file/d/1d8b_n-6fo5HbxyW-kwY3c8fWlvLOcFYy/view?usp=share_link'\n",
        " \n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        " \n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('-3-2022-.csv') \n",
        " \n",
        "df_pd = pd.read_csv('-3-2022-.csv')\n",
        "df_pd.info()"
      ],
      "metadata": {
        "id": "td2oYrWVCq9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = spark.read.csv(\"/tmp/resources/zipcodes.csv\")\n"
      ],
      "metadata": {
        "id": "lqjZGljHWsOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translate Data"
      ],
      "metadata": {
        "id": "sIp03wWjN2mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0rc1 gwpy &> /dev/null"
      ],
      "metadata": {
        "id": "MFYmGojeN8xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "translator = Translator()"
      ],
      "metadata": {
        "id": "xZwTFY1eQbDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_column (df:pd.DataFrame, name:str):\n",
        "  y = 'he_' + name\n",
        "  column_to_translate = pd.DataFrame(list(df[name].unique())).rename({0:y},axis=1)\n",
        "  x = 'en_' + name\n",
        "  column_to_translate[x] = column_to_translate[y].apply(lambda x: translator.translate(x, src='he', dest='en').text)\n",
        "  df = df.merge(column_to_translate, left_on=name, right_on=y,how ='inner')\n",
        "  df[name]=df[x]\n",
        "  df=df.drop(columns=[x,y])\n",
        "  return df"
      ],
      "metadata": {
        "id": "Ouu10Xk23lov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_df_pd = df_pd\n",
        "column_names = en_df_pd.columns\n",
        "k = column_names.delete([4,5,6,9])\n",
        "for i in k:\n",
        "  en_df_pd = translate_column(en_df_pd,i)\n",
        "en_df_pd"
      ],
      "metadata": {
        "id": "gGEmIX9LEb1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_df_pd.to_csv('/content/en_df_pd.csv', index=False)"
      ],
      "metadata": {
        "id": "o5WEC2GAnBH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Spark-DF"
      ],
      "metadata": {
        "id": "VXhwSQOybU9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .getOrCreate()\n",
        "df = spark.createDataFrame(en_df_pd.astype(str))\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "lhun1Dw6c4fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.withColumn(\"TikimSum\",df.TikimSum.cast('int'))"
      ],
      "metadata": {
        "id": "d1uTqoZ2ukiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df1.withColumn('year', substring('Quarter', 1,4))\\\n",
        "    .withColumn('Q', substring('Quarter', 6,2))\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "id": "MVVcA_rj2-Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df2.withColumn(\"year\",df2.year.cast(StringType()))"
      ],
      "metadata": {
        "id": "Ua3Z4I9q-ntR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2.withColumn(\"date\", \\\n",
        "   when((df2.Q == 'Q2'), lit(\"-30-06\"))\\\n",
        "   .when((df2.Q == 'Q3'), lit(\"-30-09\"))\\\n",
        "   .when((df2.Q == 'Q4'), lit(\"-31-12\"))\\\n",
        "   .otherwise(lit(\"-31-03\")) \\\n",
        "  )"
      ],
      "metadata": {
        "id": "HQmVydi-89rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.show(5)"
      ],
      "metadata": {
        "id": "u1wdycgZAVsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_date = df3.select(concat('year','date').alias(\"Exact Date\"))\n",
        "df3 = df3.withColumn(\"Exact Date\", concat('year','date'))\n",
        "df3.show(5)"
      ],
      "metadata": {
        "id": "jyWmBiXbUokh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df3.withColumn(\"Date Value\", to_date(df3['Exact Date'], \"yyyy-dd-MM\"))"
      ],
      "metadata": {
        "id": "J2zDOdgQNmwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.write.parquet(\"/content/df3.parquet\")"
      ],
      "metadata": {
        "id": "HNMl4evUqYV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = spark.read.parquet(\"/content/df3.parquet\")\n"
      ],
      "metadata": {
        "id": "-lnEcX-yq9ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4.show(5)\n",
        "df4.printSchema()"
      ],
      "metadata": {
        "id": "sAxZ-TZwNzr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df5 = df4.join(socio_economic_df, on='Settlement_Council', how='left')"
      ],
      "metadata": {
        "id": "n4zv6lM-Ki30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('PoliceDistrict').distinct().collect() #this is the way how to know how many unique value in the attribute"
      ],
      "metadata": {
        "id": "z74gUupTsll3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = \"https://drive.google.com/file/d/1RBkuTYpTQHl0EjhiteL3DjSLd5xUxvYa/view?usp=share_link\"\n",
        " \n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        " \n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('city_data_.csv') \n",
        " \n",
        "citydata = spark.read.csv(\"city_data_.csv\",inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "HtcPUxLjvvlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "citydata = citydata.withColumnRenamed(\"שם יישוב\",\"Settlement_Council\")\n",
        "citydata = citydata.withColumnRenamed(\"סך הכל אוכלוסייה 2021\",\"Population\")\n",
        "citydata = citydata.withColumnRenamed(\"תעתיק\" , \"City Name\")\n",
        "\n",
        "citydata.show(5)"
      ],
      "metadata": {
        "id": "xcdHlnBOxDSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.show(5)"
      ],
      "metadata": {
        "id": "p5iae2-oxQPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_df = citydata.select(\"Settlement_Council\", \"Population\",\"City Name\" )\n",
        "\n",
        "# Join the selected columns from city_df with crimes_df on the 'city_name' column\n",
        "df4 = df3.join(city_df, on='Settlement_Council', how='left')"
      ],
      "metadata": {
        "id": "oIPmsFpjxWqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4.show(5)"
      ],
      "metadata": {
        "id": "ZFBijiAHz8oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = \"https://drive.google.com/file/d/1FRVWLJLf_h-vWivkEzvC5RJBiI5kRnPW/view?usp=share_link\"\n",
        " \n",
        "# to get the id part of the file\n",
        "id = link.split(\"/\")[-2]\n",
        " \n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('s_e_2019.csv') \n",
        " \n",
        "socio_economic_df = spark.read.csv(\"s_e_2019.csv\",inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "uhv-HyVO1i4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "socio_economic_df.show(5)"
      ],
      "metadata": {
        "id": "sRk7LkoU2A68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "socio_economic_df = socio_economic_df.select(col(\"Settlement_Council\"), col(\"City_Cluster_2019\"))\n",
        "\n",
        "# Join the selected columns from city_df with crimes_df on the 'city_name' column\n",
        "df5 = df4.join(socio_economic_df, on='Settlement_Council', how='left')"
      ],
      "metadata": {
        "id": "v65gXa382Rhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = df5.withColumnRenamed(\"Settlement_Council\", \"Settlement_Council_Hebrew\")\n",
        "df5 = df5.withColumnRenamed(\"City Name\", \"Settlement_Council\")"
      ],
      "metadata": {
        "id": "NHaJm0nJaPdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.collect()[100000] ## show specific row"
      ],
      "metadata": {
        "id": "m1GFtwe_37bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.show()"
      ],
      "metadata": {
        "id": "lswWB69tcpAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.printSchema()"
      ],
      "metadata": {
        "id": "pnScov91Uxhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Analysis"
      ],
      "metadata": {
        "id": "uPEluTRgbQ-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tikim_City = df5.groupBy('Settlement_Council').sum('TikimSum')\n",
        "df_Tikim_City = df_Tikim_City.join(city_df, on='Settlement_Council', how='left')\n",
        "df_Tikim_City = df_Tikim_City.withColumn(\"Tikim by population\",col(\"sum(TikimSum)\")/ col(\"Population\"))\n",
        "df_Tikim_City = df_Tikim_City.sort(\"Tikim by population\",ascending=False)\n",
        "df_Tikim_City.select(\"Settlement_Council\",\"Tikim by population\").show(10)"
      ],
      "metadata": {
        "id": "9g7s-2qxbXxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # If the cell below doesn't work, run this:\n",
        "!python -m pip uninstall matplotlib\n",
        "!pip install matplotlib==3.1.3"
      ],
      "metadata": {
        "id": "MdmaZp1tQUoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(df_Tikim_City.toPandas()[\"Tikim by population\"]);"
      ],
      "metadata": {
        "id": "FFISMVtOP8qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Tikim_District = df5.groupBy('PoliceDistrict','Date Value').sum('TikimSum')"
      ],
      "metadata": {
        "id": "ejAd4npBkWLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_Tikim_District = df_Tikim_District.toPandas()\n",
        "pd_Tikim_District"
      ],
      "metadata": {
        "id": "lVNjiA3zd1BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "districts = pd.unique(pd_Tikim_District['PoliceDistrict'])\n",
        "for d in districts:\n",
        "  pd_district = pd_Tikim_District[pd_Tikim_District['PoliceDistrict'] == d]\n",
        "  pd_district = pd_district.sort_values(by = 'Date Value')\n",
        "  plt.plot(pd_district['Date Value'],pd_district['sum(TikimSum)'])\n",
        "plt.legend(districts, loc=\"upper right\")\n",
        "plt.axvline(x=datetime.date(2020,4,8), color='black', linestyle='dashed')\n",
        "plt.axvline(x=datetime.date(2021,3,31), color='black', linestyle='dashed')\n",
        "plt.ylim([0,25000])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Emv9ewxNltPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tikim_district = df5.groupBy('PoliceDistrict','StatisticCrimeGroup','Date Value').sum('TikimSum')\n",
        "# df_tikim_district = df5.join(df_tikim_district, on='StatisticCrimeGroup', how='left')\n",
        "pd_tikim_district = df_tikim_district.where(df_tikim_district.PoliceDistrict == \"Central district\").toPandas()"
      ],
      "metadata": {
        "id": "FReR4XuuFZz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "types = pd.unique(pd_tikim_district['StatisticCrimeGroup'])\n",
        "for t in types:\n",
        "  pd_type = pd_tikim_district[pd_tikim_district['StatisticCrimeGroup'] == t]\n",
        "  pd_type = pd_type.sort_values(by = 'Date Value')\n",
        "  plt.plot(pd_type['Date Value'],pd_type['sum(TikimSum)'])\n",
        "plt.legend(types, loc=\"upper right\")\n",
        "plt.axvline(x=datetime.date(2020,4,8), color='black', linestyle='dashed')\n",
        "plt.axvline(x=datetime.date(2021,3,31), color='black', linestyle='dashed')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Y7cN3b5FyYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "district = [\"Southern District\", \"Shay County\", \"Jerusalem District\", \"North District\", \"Beach district\", \"Central district\", \"Cell County\"]\n",
        "for i in range(7):\n",
        "  pd_tikim_district = df_tikim_district.where(df_tikim_district.PoliceDistrict == district[i]).toPandas()\n",
        "  types = pd.unique(pd_tikim_district['StatisticCrimeGroup'])\n",
        "  for t in types:\n",
        "    pd_type = pd_tikim_district[pd_tikim_district['StatisticCrimeGroup'] == t]\n",
        "    pd_type = pd_type.sort_values(by = 'Date Value')\n",
        "    plt.plot(pd_type['Date Value'],pd_type['sum(TikimSum)'])\n",
        "  plt.legend(types, loc = \"upper right\")\n",
        "  plt.axvline(x=datetime.date(2020,4,8), color='black', linestyle='dashed')\n",
        "  plt.axvline(x=datetime.date(2021,3,31), color='black', linestyle='dashed')\n",
        "  plt.title(district[i])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "On1CwJmWI_JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establishing \"Crime Score\""
      ],
      "metadata": {
        "id": "MYW2895Febyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crime_types = df5.select('StatisticCrimeGroup').distinct()\n",
        "crime_types = crime_types.withColumn('Score',col('StatisticCrimeGroup'))\n",
        "df5.select('StatisticCrimeGroup').distinct().show(17,False)"
      ],
      "metadata": {
        "id": "Z-PQ2yntpY3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crime_scores = {'The rest of the offenses':'1', 'Traffic violations': '2', 'Nan': '3', 'Permitted offenses': '1', 'Offenses against a person':'5', 'Confidence offenses': '3', 'Sex offenses': '5',\\\n",
        "                'Administrative offenses': '1', 'Economic offenses': '2', 'Public order offenses': '1', 'Offenses toward property': '3','Setup Sections': '1',\\\n",
        "                'Fraudulent offenses': '3','Offenses against body': '5','Offenses toward morality': '4','Unknown': '3', 'The rest of the rest': '1'}\n",
        "crime_types_score = crime_types.replace(crime_scores,subset=['Score'])\n",
        "crime_types_score.sort('Score',ascending=False).show(17,False)\n"
      ],
      "metadata": {
        "id": "eICUdvyMRpUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.where(df5['StatisticCrimeGroup'] == 'Unknown').count()"
      ],
      "metadata": {
        "id": "XhOoBhJ5PjpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# crime_scores = {'שאר עבירות':'1', 'עבירות תנועה': '2', 'nan': '3', 'עבירות רשוי': '1', 'עבירות נגד אדם':'5', 'עבירות בטחון': '3', 'עבירות מין': '5',\\\n",
        "#                 'עבירות מנהליות': '1', 'עבירות כלכליות': '2', 'עבירות סדר ציבורי': '1', 'עבירות כלפי הרכוש': '3','סעיפי הגדרה': '1',\\\n",
        "#                 'עבירות מרמה': '3','עבירות נגד גוף': '5','עבירות כלפי המוסר': '4','לא ידוע': '1', 'קבוצת כל השאר': '1'}\n",
        "# crime_types_score = crime_types.replace(crime_scores,subset=['Score'])\n",
        "# crime_types_score.show()\n"
      ],
      "metadata": {
        "id": "zayaiSgKq0WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$P = $ the population number of the city\n",
        "\n",
        "$n_i = $ number of specific crime group cases\n",
        "\n",
        "$S_i = $ the score of the specific crime group\n",
        "\n",
        "$$CrimeScore = \\frac{1}{P}\\sum_{i}{n_{i}\\cdot S_{i}}$$"
      ],
      "metadata": {
        "id": "MXMiCPrvifd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df6 = df5.join(crime_types_score, on='StatisticCrimeGroup', how='left')\n",
        "df6 = df6.withColumn(\"Total_Score\",col(\"TikimSum\") * col(\"Score\"))\n",
        "df6 = df6.groupBy('Settlement_Council').sum('Total_Score')\n",
        "df_score_city = df5.join(df6, on='Settlement_Council', how='left')\n",
        "df_score_city = df_score_city.withColumn(\"Score per population\",col(\"sum(Total_Score)\") / col(\"Population\"))"
      ],
      "metadata": {
        "id": "azqIPQIt1MgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_score_city.select(\"Settlement_Council\",'City_Cluster_2019',\"Score per population\").distinct().sort(\"Score per population\",ascending=False).show(10)"
      ],
      "metadata": {
        "id": "UIcgqsKR4zNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_score_city.select('Settlement_Council','Population').filter(df_score_city.Population.isNull()).distinct().show(100)"
      ],
      "metadata": {
        "id": "5dRB2lFH797D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_score = df_score_city.select('Settlement_Council','City_Cluster_2019','Score per population').distinct().toPandas()\n",
        "sns.regplot(pd_score['City_Cluster_2019'],pd_score['Score per population'],ci=0,line_kws={\"color\": \"red\"})"
      ],
      "metadata": {
        "id": "zvXAdDpd71Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(pd_score['Score per population']);"
      ],
      "metadata": {
        "id": "h5QMAg0vPEem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$N = $ overall number of cases in the city \n",
        "\n",
        "$n_i = $ number of specific crime group cases\n",
        "\n",
        "$S_i = $ the score of the specific crime group\n",
        "\n",
        "$$CrimeScore = \\frac{1}{N}\\sum_{i}{n_{i}\\cdot S_{i}}$$"
      ],
      "metadata": {
        "id": "bIvm73tVf2SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df7 = df5.join(crime_types_score, on='StatisticCrimeGroup', how='left')\n",
        "df7 = df7.withColumn(\"Total_Score\",col(\"TikimSum\") * col(\"Score\"))\n",
        "df8 = df7.groupBy('Settlement_Council').sum('Total_Score')\n",
        "df9 = df7.groupBy('Settlement_Council').sum('TikimSum')\n",
        "df_score_city2 = df9.join(df8, on='Settlement_Council', how='left')\n",
        "df_score_city2 = df_score_city2.withColumn(\"Score per population2\",col(\"sum(Total_Score)\") / col(\"sum(TikimSum)\"))"
      ],
      "metadata": {
        "id": "91_hzObB_5F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_score_city2.select(\"Settlement_Council\",\"Score per population2\").distinct().sort(\"Score per population2\",ascending=False).show(10)"
      ],
      "metadata": {
        "id": "gS3H8SKeAqXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_score_city2 = df_score_city2.join(df5.select('Settlement_Council','City_Cluster_2019'), on='Settlement_Council', how='left')"
      ],
      "metadata": {
        "id": "dxk3_pLHBXTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_score2 = df_score_city2.select('Settlement_Council','City_Cluster_2019','Score per population2').distinct().toPandas()\n",
        "sns.regplot(pd_score2['City_Cluster_2019'],pd_score2['Score per population2'],ci=0,line_kws={\"color\": \"red\"})"
      ],
      "metadata": {
        "id": "Gx3wJedBBX0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(pd_score2['Score per population2']);"
      ],
      "metadata": {
        "id": "uuHXpSbwPVDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$P = $ the population number of the city\n",
        "\n",
        "$N = $ overall number of cases in the city \n",
        "\n",
        "$n_i = $ number of specific crime group cases\n",
        "\n",
        "$S_i = $ the score of the specific crime group\n",
        "\n",
        "$$CrimeScore = \\frac{1}{2} \\cdot (\\frac{1}{P}\\sum_{i}{n_{i}\\cdot S_{i}} + \\frac{1}{N}\\sum_{i}{n_{i}\\cdot S_{i}})$$\n"
      ],
      "metadata": {
        "id": "oHMBDqLiQm3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_score_city3 = df_score_city.select(\"Settlement_Council\",\"Score per population\").join(df_score_city2, on=\"Settlement_Council\", how=\"left\")\n",
        "# df_score_city3 = df_score_city3.withColumn(\"Average Crime Score\",(col(\"Score per population\") + col(\"Score per population2\"))/2)\n",
        "# pd_score3 = df_score_city3.select('City_Cluster_2019','Average Crime Score').distinct().toPandas()\n",
        "# sns.regplot(pd_score3['City_Cluster_2019'],pd_score3['Average Crime Score'],ci=0,line_kws={\"color\": \"red\"})"
      ],
      "metadata": {
        "id": "bkTqf2e3Oi3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deeper analysis and prediction"
      ],
      "metadata": {
        "id": "B7txVhQwjBXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building data that will fit prediction over time.\n",
        "years = ['2017', '2018', '2019', '2020', '2021', '2022']\n",
        "quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
        "df_ml = df5.select(\"Settlement_Council\",\"Population\",\"City_Cluster_2019\").distinct().dropna()\n",
        "for year in years:\n",
        "  for q in quarters:\n",
        "    if year == '2022' and q == 'Q4': # Skipping because there is no data for this time\n",
        "      break\n",
        "    year_q = year+\"_\"+q\n",
        "    df_by_time = df5.groupBy('Settlement_Council','year','Q').sum('TikimSum')\n",
        "    df_by_time = df_by_time.withColumnRenamed('sum(TikimSum)',year_q)\n",
        "    df_temp = df_by_time.where(df_by_time.year == year)\n",
        "    df_temp = df_temp.where(df_by_time.Q == q)\n",
        "    df_ml = df_ml.join(df_temp.select('Settlement_Council',year_q),on='Settlement_Council', how='left')"
      ],
      "metadata": {
        "id": "n0uUhr_x2zXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ml.show(5)"
      ],
      "metadata": {
        "id": "5MY0CnqJ_DTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "metadata": {
        "id": "OMwlKoUarGGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_columns = df_ml.columns[1:-1] # Removing the city name and the last quarter of 2022 (\"2022-Q3\") from the training columns"
      ],
      "metadata": {
        "id": "NIGz-1vixZC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a feature vector by combining the columns\n",
        "df_ml = df_ml.dropna()\n",
        "vector_assembler = VectorAssembler(inputCols=training_columns, outputCol=\"features\", handleInvalid=\"keep\")\n",
        "df_ml_vec = vector_assembler.transform(df_ml)"
      ],
      "metadata": {
        "id": "q5FDP6d5vMfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "df_ml_data = df_ml_vec.select(\"Settlement_Council\",\"features\",\"2022_Q3\")\n",
        "training_data, testing_data = df_ml_data.randomSplit([0.8, 0.2])"
      ],
      "metadata": {
        "id": "zSMuHszwvRVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the linear regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"2022_Q3\")\n",
        "model = lr.fit(training_data)"
      ],
      "metadata": {
        "id": "-aSRhiJgy6Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the model to make predictions on the testing data\n",
        "predictions = model.transform(testing_data)\n",
        "predictions.show(5)"
      ],
      "metadata": {
        "id": "kQA3Y1jY0LWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol='2022_Q3')\n",
        "evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "fUreaAz3EoyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2023 = df_ml.dropna()\n",
        "training_columns = df_ml.columns[2:] # Now we will look at data data from one quarter later to adjust the model.\n",
        "vector_assembler_2023 = VectorAssembler(inputCols=training_columns, outputCol=\"features\" ,handleInvalid=\"keep\")\n",
        "df_2023 = vector_assembler_2023.transform(df_2023).select(\"Settlement_Council\",\"features\",\"2022_Q3\")\n",
        "predictions_2023 = model.transform(df_2023)"
      ],
      "metadata": {
        "id": "noFKyR8xHYlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_2023 = predictions_2023.withColumn(\"Crime Increase\",(col(\"prediction\")-col(\"2022_Q3\"))/col(\"2022_Q3\"))"
      ],
      "metadata": {
        "id": "cx6xKAhrJIU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The cities with the predicted crime increase for 2023:\")\n",
        "predictions_2023.sort(\"Crime Increase\",ascending=False).select(\"Settlement_Council\",\"Crime Increase\").show()"
      ],
      "metadata": {
        "id": "aKAnXoTcKbqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ניסיונות כושלים"
      ],
      "metadata": {
        "id": "T7ouxD3obf9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## try to translate directly to the SPARK with a DICT"
      ],
      "metadata": {
        "id": "BBfUroneFmVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "היתרון בספרייה הוא שניתן לערוך את התרגום לפני הרצה. אני מניח שניתן לעשות את זה בפנדה\n"
      ],
      "metadata": {
        "id": "5s6MvRIxa28I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dic (df:pd.DataFrame, name:str):\n",
        "  y = 'he_' + name\n",
        "  column_to_translate = pd.DataFrame(list(df[name].unique())).rename({0:y},axis=1)\n",
        "  x = 'en_' + name\n",
        "  column_to_translate[x] = column_to_translate[y].apply(lambda x: translator.translate(x, src='he', dest='en').text)\n",
        "  dic = dict(zip(column_to_translate[x], column_to_translate[y]))\n",
        "  return dic\n"
      ],
      "metadata": {
        "id": "7_eEMf7tW8Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_dic (df_pd,'StatisticCrimeGroup')"
      ],
      "metadata": {
        "id": "XBxKrRs9chKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "# Create a new column called \"new_column\"\n",
        "df = df.withColumn(\"new_column\", when(col(\"StatisticCrimeGroup\").isin(dic_StatisticCrimeGroup.keys()),\\\n",
        "                                      col(\"StatisticCrimeGroup\").cast(\"string\").map(dic_StatisticCrimeGroup)).otherwise(col(\"StatisticCrimeGroup\")))"
      ],
      "metadata": {
        "id": "5cZPqGzsohFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2 = df.rdd.map(lambda x: (x.ArealAttributionType,x.PoliceDistrict,x.PoliceMerhav,x.PoliceStation,x.Settlement_Council,x.StatArea,x.Quarter, \\\n",
        "                            dic_StatisticCrimeGroup[x.StatisticCrimeGroup],x.StatisticCrimeType,x.TikimSum))"
      ],
      "metadata": {
        "id": "B4C0ROIPY_jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2.collect()"
      ],
      "metadata": {
        "id": "qFT-TAvFmN_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "                            .toDF([\"Area Attribution Type\",\"Police District\",\"Police Merhav\",\"Police Station\",\"Settlement Council\",\"Stat Area\",\"Quarter\",\\\n",
        "                                   \"Statistic Crime Group\",\"Statistic Crime Type\",\"Tikim Sum\"])\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "MQ14o8W0l5z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try to translate with spark translate func"
      ],
      "metadata": {
        "id": "GUcVQ8VXGP-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client"
      ],
      "metadata": {
        "id": "ztCeCqIhrlbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import translate"
      ],
      "metadata": {
        "id": "ftpF4IiXrrFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"test\", translate(col(\"StatisticCrimeGroup\"), \"HEBREW\", \"ENGLISH\"))"
      ],
      "metadata": {
        "id": "Q80J3yA0rsnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unique = df.select(\"test\").distinct()\n",
        "df_unique.show()"
      ],
      "metadata": {
        "id": "vfP07Ldyr8QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WUwLx2KwWIwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HW2 - remembers"
      ],
      "metadata": {
        "id": "qP-tclhENY_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RDD Start Questions"
      ],
      "metadata": {
        "id": "VMP32olUiqW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create RDD from list"
      ],
      "metadata": {
        "id": "J4Zz9RDQmPUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rdd_from_list(sc: SparkContext, n: int) -> RDD:  \n",
        "    \"\"\"\n",
        "    This function gets the number n and returns a rdd consisting of elements from 1 to n.\n",
        "    Assume that n > 1, no need for any exeption on that manner.\n",
        "    input:\n",
        "        sc: spark context\n",
        "        n: the top number in the generated rdd. For example, if n=3 the rdd created should be [1,2,3]\n",
        "    output:\n",
        "        returns a rdd consisting of elements from 1 to n (includes n).\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    data = list(range(1,n+1))\n",
        "    rdd = sc.parallelize(data)\n",
        "    return rdd\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "3Lwyuk6PmPKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When running this cell your code should not return any assert errors\n",
        "\n",
        "# collect() method returns all elements in a RDD to the driver as a local list\n",
        "print(rdd_from_list(sc, 10).collect())\n",
        "\n",
        "result_rdd = rdd_from_list(sc, 3)\n",
        "\n",
        "assert isinstance(result_rdd, RDD)\n",
        "assert result_rdd.collect() == [1, 2, 3]"
      ],
      "metadata": {
        "id": "36ZE25A0mObw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using key-value tuple, square all the values.\n",
        "You can assume that the key is str (String) type and the value is int (integer) type"
      ],
      "metadata": {
        "id": "DlX48EsljI-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def square_list(sc: SparkContext, pair_list: list) -> list:\n",
        "    \"\"\"\n",
        "    This function gets pair of tuples and returns a list with squre values\n",
        "    input:\n",
        "        sc: spark context\n",
        "        pair_list: a pair key-value tuple that contain string as a key and integer as a value \n",
        "    output:\n",
        "        return a list of key and square values of the original list\n",
        "    \"\"\"\n",
        "    # You must use PySpark RDD in part of this solution\n",
        "    # Your code here\n",
        "    rdd = sc.parallelize(pair_list)\n",
        "    squared_rdd = rdd.map(lambda x: (x[0], x[1]**2))\n",
        "    squared_values = squared_rdd.collect()\n",
        "    return squared_values\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "qQxwbD2Si_cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# When running this cell your code should not return any assert errors\n",
        "\n",
        "pair_list = [('a', 1), ('b', 2), ('c', 3)]\n",
        "assert square_list(sc, pair_list) == [('a', 1), ('b', 4), ('c', 9)]"
      ],
      "metadata": {
        "id": "cJFfW0JejCXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alter Matrix\n"
      ],
      "metadata": {
        "id": "5A0BI0yToVcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have a rdd containing only lists of 2 elements:\n",
        "\n",
        "```\n",
        "matrix = [[1,5], [2,10], [3,14]]\n",
        "matrix_rdd = sc.parallelize(matrix)\n",
        "```\n",
        "\n",
        "This data structure is like a matrix.\n",
        "\n",
        "Create an operation `alter_matrix()` which adding to the first column (or first coordinate of each element) of the matrix by 1, and integer divide 5 from the second column (second coordinate)."
      ],
      "metadata": {
        "id": "HQStr4cfGxxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc.parallelize([[1,3], [2,9]]).map(lambda row: row[0]).collect()"
      ],
      "metadata": {
        "id": "7ymOU1Kgpg0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alter_matrix(sc: SparkContext, matrix: RDD) -> list:\n",
        "    \"\"\"\n",
        "    This function adds 1 for the first column, and integer divide 5 from the second column \n",
        "    input:\n",
        "        sc: spark context\n",
        "        matrix: a RDD that contain data a list in length of two.\n",
        "    output:\n",
        "        return a RDD adding 1 for the first column and integer divide 5 from the second column of the input RDD\n",
        "    \"\"\"\n",
        "    # You must use PySpark RDD in part of this solution\n",
        "    # Your code here\n",
        "    alterned_rdd = matrix.map(lambda x: [x[0]+1, x[1]//5])\n",
        "    return alterned_rdd\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "nlblY_UroXUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = [[1,5], [2,10], [3,14]]\n",
        "matrix_rdd = sc.parallelize(matrix)\n",
        "result_rdd = alter_matrix(sc, matrix_rdd)\n",
        "\n",
        "assert isinstance(result_rdd, RDD)\n",
        "assert result_rdd.collect() == [[2, 1], [3, 2], [4, 2]]"
      ],
      "metadata": {
        "id": "vn2OASoboXNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gets even numbers\n",
        "Assume we have a RDD containin integer numbers.\n",
        "Create a function `gets_even()` which returns all the even numbers in the RDD."
      ],
      "metadata": {
        "id": "iYrsXJVZqdsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gets_even(sc: SparkContext, number_rdd: RDD) -> list:\n",
        "    \"\"\"\n",
        "    This function gets rdd and return only even numbers from the given rdd\n",
        "    input:\n",
        "        sc: spark context\n",
        "        number_rdd: a RDD that contain data of integer numbers\n",
        "    output:\n",
        "        return a RDD that contain only even numbers\n",
        "    \"\"\"\n",
        "    # You must use PySpark RDD in part of this solution\n",
        "    # Your code here\n",
        "    even_number_rdd = number_rdd.filter(lambda x: x % 2 == 0)\n",
        "    return even_number_rdd\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "P4j-lMidqf1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = [i for i in range(1, 10)]\n",
        "numbers_rdd = sc.parallelize(numbers)\n",
        "result_rdd = gets_even(sc, numbers_rdd)\n",
        "\n",
        "assert isinstance(result_rdd, RDD)\n",
        "assert result_rdd.collect() == [2,4,6,8]"
      ],
      "metadata": {
        "id": "iQKaMvcqrHQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Page Rank algorithm\n"
      ],
      "metadata": {
        "id": "YT-ceD4JsDll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PageRank algorithm assigns a real number to each page on the web (or the portion of the web that has been crawled and its links discovered). This number is meant to indicate how \"important\" the page is.<br> Hadoop was initially developed for Apache Nutch, an open-source web search engine, and one of the first uses of Big Data technologies and MapReduce was to index millions of webpages. In this application, we will explore an implementation of the iterative PageRank algorithm using Spark."
      ],
      "metadata": {
        "id": "WSKQWzRBBT7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will deal with a more simplifed web system. Assume we have four web pages (A,B,C,D), the following graph shows outbound links from one web page to another (the arrow indicate the flow)<br>\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "vO61L21VCIhc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate a web system as an RDD of pair tuples (page name, neighbor page name) from a pair rdd (page name, list of all neighbors)\n",
        "Hint: check out for pair rdd opearations such as, `mapValues` or `flatMapValues`. [Spark reference](https://spark.apache.org/docs/2.2.0/api/python/pyspark.html#pyspark.RDD)"
      ],
      "metadata": {
        "id": "nWkSC7w6DmYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_websystem_rdd(sc: SparkContext, neighbor_rdd: RDD) -> RDD:\n",
        "    \"\"\"\n",
        "    This function gets input web system as rdd and mapping rdd of the system.\n",
        "    And returns the mapping of the connection between each two web pages\n",
        "    input:\n",
        "        sc: spark context\n",
        "        neighbor_rdd: a pair RDD of the web system contain each web and its links (web page, list of neighbor pages)\n",
        "    output:\n",
        "        return a pair RDD that contain all links (web page, neighbor web page)\n",
        "    \"\"\"\n",
        "    # You must use PySpark RDD in part of this solution\n",
        "    # Your code here\n",
        "    def get_link_pairs(x):\n",
        "        page = x[0]\n",
        "        neighbors = x[1]\n",
        "        pairs = [(page, neighbor) for neighbor in neighbors]\n",
        "        return pairs\n",
        "    websystem_rdd = neighbor_rdd.flatMap(get_link_pairs)\n",
        "    return websystem_rdd\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "noJZrL2PCnc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neighbor_rdd = sc.parallelize([\n",
        "                ('a', ['b','c','d']),\n",
        "                ('c', ['b']), \n",
        "                ('b', ['c','d']), \n",
        "                ('d', ['a','c'])\n",
        "              ])"
      ],
      "metadata": {
        "id": "YXThH2iRsI8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = map_websystem_rdd(sc, neighbor_rdd).collect()"
      ],
      "metadata": {
        "id": "YCDGfWthsI3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert result == [\n",
        "                    ('a', 'b'), ('a', 'c'), ('a', 'd'), \n",
        "                    ('c', 'b'), \n",
        "                    ('b', 'c'), ('b', 'd'), \n",
        "                    ('d', 'a'), ('d', 'c')\n",
        "                ]"
      ],
      "metadata": {
        "id": "-yugWjG-sI02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kaggle dataset analysis"
      ],
      "metadata": {
        "id": "p8SF5yQ1gdn-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can choose whatever data set you like from Kaggle and you need to derive meaningful insights using the Pyspark Dataframe API.\n",
        "You need to use all the following methods at least one time [DataFrame API](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame):\n",
        "\n",
        "*   [show](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.show.html#pyspark.sql.DataFrame.show)\n",
        "*   [printSchema](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.printSchema.html#pyspark.sql.DataFrame.printSchema)\n",
        "*   [distinct](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.distinct.html#pyspark.sql.DataFrame.distinct)\n",
        "*   [filter](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.filter.html#pyspark.sql.DataFrame.filter)\n",
        "*   [foreach](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.foreach.html#pyspark.sql.DataFrame.foreach)\n",
        "*   [join](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.join.html#pyspark.sql.DataFrame.join)\n",
        "*   [orderBy](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html#pyspark.sql.DataFrame.orderBy)\n",
        "*   [sample](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.sample.html#pyspark.sql.DataFrame.sample)\n",
        "*   [select](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.select.html#pyspark.sql.DataFrame.select)\n",
        "*   [subtract](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.subtract.html#pyspark.sql.DataFrame.subtract)\n",
        "*   [where](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.where.html#pyspark.sql.DataFrame.where)\n",
        "*   [withColumn](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.withColumn.html#pyspark.sql.DataFrame.withColumn)\n",
        "*   [withColumnRenamed](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.withColumnRenamed.html#pyspark.sql.DataFrame.withColumnRenamed)\n",
        "\n",
        "In addition, use at least one of the given [functions](https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.sql.html#functions):\n",
        "*   [array_contains](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.array_contains.html#pyspark.sql.functions.array_contains)\n",
        "*   [avg](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.avg.html#pyspark.sql.functions.avg)\n",
        "*   [collect_set](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.collect_set.html#pyspark.sql.functions.collect_set)\n",
        "*   [countDistinct](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.countDistinct.html#pyspark.sql.functions.countDistinct)\n",
        "*   [datediff](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.datediff.html#pyspark.sql.functions.datediff)\n",
        "*   [min](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.min.html#pyspark.sql.functions.min)\n",
        "*   [regexp_extract](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.regexp_extract.html#pyspark.sql.functions.regexp_extract)\n",
        "*   [window](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.functions.window.html?highlight=window)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E8gEFxpJHUC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Close spark session"
      ],
      "metadata": {
        "id": "3HhGXovmmz_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "nbngjjhtm1uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FzA1JrJ5KI4v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}